{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e84ce8-7809-4b40-b164-e4a5df76ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "import pickle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0943f790-4a5c-4ecd-b438-9275a284de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2999/1419747308.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(np.logical_or(Y_data.y, Y_data.y_te)).type(torch.FloatTensor).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#load the required data for the model\n",
    "data = torch.load('../data/pan/string_850/data_67_0.0001_2.0.pkl')\n",
    "data = data.type(torch.FloatTensor).to(device)\n",
    "\n",
    "data2 = torch.load('../data/pan/string_850/data_67_0.0001_go_2.0.pkl')\n",
    "data2 = data2.type(torch.FloatTensor).to(device)\n",
    "\n",
    "data3 = torch.load('../data/pan/string_850/data_67_0.0001_path_2.0.pkl')\n",
    "data3 = data3.type(torch.FloatTensor).to(device)\n",
    "\n",
    "Y_data = torch.load('../data/pan/string_850/Y_796_data_2.0.pkl')\n",
    "Y = torch.tensor(np.logical_or(Y_data.y, Y_data.y_te)).type(torch.FloatTensor).to(device)\n",
    "\n",
    "with open(\"../data/pan/string_850/k_sets_796_2.0.pkl\", 'rb') as handle:\n",
    "    k_sets = pickle.load(handle)\n",
    "    \n",
    "e_data = torch.load(\"../data/pan/string_850/mut_data_miRNA_sub_du_go_path_2.0.pkl\")\n",
    "e_edge_index =e_data.edge_index\n",
    "e_edge_index = e_edge_index.to(device)\n",
    "\n",
    "#transformer layer 1 parameter settings\n",
    "d_model1 = 67  #dimension of input features\n",
    "d_ff1 = 268  #dimension mapped by feedforward neural network\n",
    "d_k1 = d_v1 = 67  # dimension of K(=Q), V\n",
    "\n",
    "#transformer layer 2 parameter settings\n",
    "d_model2 = 300  #dimension of input features\n",
    "d_ff2 = 1200  #dimension mapped by feedforward neural network\n",
    "d_k2 = d_v2 = 300  # dimension of K(=Q), V\n",
    "\n",
    "n_heads = 6  #number of heads in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4dd0ac7-da2d-4699-b847-ce77a54d8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the framwork of Transformer layer 1\n",
    "class ScaledDotProductAttention1(nn.Module):\n",
    "    #use a single attention head to aggregate information\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention1, self).__init__()\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k1)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention1(nn.Module):\n",
    "    #use the multi-head attention mechanism to calculate the feature representations \n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention1, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model1, d_k1 * n_heads) \n",
    "        self.W_K = nn.Linear(d_model1, d_k1 * n_heads)\n",
    "        self.W_V = nn.Linear(d_model1, d_v1 * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v1, d_model1)\n",
    " \n",
    "    def forward(self, H):\n",
    "        residual, batch_size = H, H.size(0)\n",
    "        q_s = self.W_Q(H).view(batch_size, -1, n_heads, d_k1).transpose(1,2)\n",
    "        k_s = self.W_K(H).view(batch_size, -1, n_heads, d_k1).transpose(1,2)\n",
    "        v_s = self.W_V(H).view(batch_size, -1, n_heads, d_v1).transpose(1,2)\n",
    " \n",
    "        context = ScaledDotProductAttention1()(q_s, k_s, v_s)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v1)\n",
    "        output = self.linear(context)\n",
    "        return torch.relu(output + residual) #residual connection\n",
    "    \n",
    "class PoswiseFeedForwardNet1(nn.Module):\n",
    "    #the feedforward neural network of transformer layer 1\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet1, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model1, d_ff1, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(d_ff1, 300, bias=False))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.fc(inputs)\n",
    "        return output\n",
    "\n",
    "class TransformerLayer1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerLayer1, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention1()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet1()\n",
    " \n",
    "    def forward(self, enc_inputs):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs\n",
    "\n",
    "class TL1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TL1, self).__init__()\n",
    "        self.trans = TransformerLayer1()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        trans_inputs = inputs.unsqueeze(1)\n",
    "        trans_outputs = self.trans(trans_inputs)\n",
    "        outputs = trans_outputs.view(10743,-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7d7a09-0542-4cce-85d7-9f8d048a761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the framwork of Transformer layer 2\n",
    "class ScaledDotProductAttention2(nn.Module):\n",
    "    #use a single attention head to aggregate information\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention2, self).__init__()\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k2)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention2(nn.Module):\n",
    "    #use the multi-head attention mechanism to calculate the feature representations\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention2, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model2, d_k2 * n_heads) \n",
    "        self.W_K = nn.Linear(d_model2, d_k2 * n_heads)\n",
    "        self.W_V = nn.Linear(d_model2, d_v2 * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v2, d_model2)\n",
    " \n",
    "    def forward(self, H):\n",
    "        residual, batch_size = H, H.size(0)\n",
    "        q_s = self.W_Q(H).view(batch_size, -1, n_heads, d_k2).transpose(1,2)\n",
    "        k_s = self.W_K(H).view(batch_size, -1, n_heads, d_k2).transpose(1,2)\n",
    "        v_s = self.W_V(H).view(batch_size, -1, n_heads, d_v2).transpose(1,2)\n",
    " \n",
    "        context = ScaledDotProductAttention2()(q_s, k_s, v_s)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v2)\n",
    "        output = self.linear(context)\n",
    "        return torch.relu(output + residual) #residual connection\n",
    "\n",
    "class PoswiseFeedForwardNet2(nn.Module):\n",
    "    #the feedforward neural network of transformer layer 2\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet2, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model2, d_ff2, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(d_ff2, 100, bias=False))\n",
    "        self.lin = Linear(300,100)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        residual = self.lin(inputs)\n",
    "        output = self.fc(inputs)\n",
    "        return torch.relu(output + residual) #residual enhanced activation\n",
    "\n",
    "class TransformerLayer2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerLayer2, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention2()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet2()\n",
    " \n",
    "    def forward(self, enc_inputs):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs\n",
    "\n",
    "class TL2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TL2, self).__init__()\n",
    "        self.trans = TransformerLayer2()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        trans_inputs = inputs.unsqueeze(1)\n",
    "        trans_outputs = self.trans(trans_inputs)\n",
    "        outputs = trans_outputs.view(10743,-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f73319-abe1-43f0-af8a-085d8c3f52c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.T11 = TL1()\n",
    "        self.T12 = TL2()\n",
    "        \n",
    "        self.T21 = TL1()\n",
    "        self.T22 = TL2()\n",
    "        \n",
    "        self.T31 = TL1()\n",
    "        self.T32 = TL2()\n",
    "        \n",
    "        self.GNN = ChebConv(100, 1, K=2, normalization=\"sym\")\n",
    "\n",
    "        self.lin11 = Linear(67, 300)\n",
    "        self.lin21 = Linear(67, 300)\n",
    "        self.lin31 = Linear(67, 300)\n",
    "        \n",
    "    def forward(self):\n",
    "        edge_index, _ = dropout_edge(e_edge_index, p=0.5,force_undirected=True,training=self.training)\n",
    "        x01 = F.dropout(data, training=self.training)\n",
    "        x02 = F.dropout(data2, training=self.training)\n",
    "        x03 = F.dropout(data3, training=self.training)\n",
    "        \n",
    "        #learn genes feature representations across various gene association networks by the transformer module\n",
    "        x1 = self.T11(x01)\n",
    "        x2 = torch.relu(x1 + self.lin11(x01)) #residual enhanced activation\n",
    "        x3 = F.dropout(x2, training=self.training)\n",
    "        x4 = self.T12(x3)\n",
    "        \n",
    "        x6 = self.T21(x02)\n",
    "        x7 = torch.relu(x6 + self.lin21(x02)) #residual enhanced activation\n",
    "        x8 = F.dropout(x7, training=self.training)\n",
    "        x9 = self.T22(x8)\n",
    "        \n",
    "        x11 = self.T31(x03)\n",
    "        x12 = torch.relu(x11 + self.lin31(x03)) #residual enhanced activation\n",
    "        x13 = F.dropout(x12, training=self.training)\n",
    "        x14 = self.T32(x13)\n",
    "        \n",
    "        #aggregate the three feature representation matrices and obtain predicted results by Chebyshev GCN\n",
    "        x19 = x4 + x9 + x14\n",
    "        x20 = F.dropout(x19, training=self.training)\n",
    "        x21 = self.GNN(x20, edge_index)\n",
    "        \n",
    "        return x21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87afce7f-0c45-42b7-977f-4c1411e08544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model()\n",
    "    loss = F.binary_cross_entropy_with_logits(pred[mask], Y[mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    x = model()\n",
    "    pred = torch.sigmoid(x[mask]).cpu().detach().numpy()\n",
    "    Yn = Y[mask].cpu().numpy()\n",
    "    precision, recall, _thresholds = metrics.precision_recall_curve(Yn, pred)\n",
    "    area = metrics.auc(recall, precision)\n",
    "    return metrics.roc_auc_score(Yn, pred), area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f535a-7749-423d-b480-c04be1133fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 550 \n",
    "time_start = time.time()\n",
    "#ten five-fold cross-validations\n",
    "AUC = np.zeros(shape=(10, 5))\n",
    "AUPR = np.zeros(shape=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    for cv_run in range(5):\n",
    "        tr_mask = k_sets[i][cv_run][0]\n",
    "        te_mask = k_sets[i][cv_run][1]\n",
    "        model = net().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "        for t in range(epochs):\n",
    "            train(tr_mask)\n",
    "    \n",
    "        AUC[i][cv_run], AUPR[i][cv_run] = test(te_mask) \n",
    "        print(\"round %d and %d times cross-validations:\" %(i+1,cv_run+1))\n",
    "        print(AUC[i][cv_run], AUPR[i][cv_run])\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(time.time() - time_start)\n",
    "print(AUC.mean())\n",
    "print(AUPR.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myapp",
   "language": "python",
   "name": "myapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
