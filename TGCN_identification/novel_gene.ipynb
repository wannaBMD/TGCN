{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6565c7-0239-41ea-87c0-5f4364fb30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.utils import dropout_edge\n",
    "import pickle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5be685f-7a63-431b-a588-d6bb1c49bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1246/699151247.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y = torch.tensor(np.logical_or(Y_data.y, Y_data.y_te)).type(torch.FloatTensor).to(device)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = torch.load('../data/pan/string_850/data_67_0.0001_2.0.pkl')\n",
    "data = data.type(torch.FloatTensor).to(device)\n",
    "\n",
    "data2 = torch.load('../data/pan/string_850/data_67_0.0001_go_2.0.pkl')\n",
    "data2 = data2.type(torch.FloatTensor).to(device)\n",
    "\n",
    "data3 = torch.load('../data/pan/string_850/data_67_0.0001_path_2.0.pkl')\n",
    "data3 = data3.type(torch.FloatTensor).to(device)\n",
    "\n",
    "Y_data = torch.load('../data/pan/string_850/Y_796_data_2.0.pkl')\n",
    "Y = torch.tensor(np.logical_or(Y_data.y, Y_data.y_te)).type(torch.FloatTensor).to(device)\n",
    "\n",
    "with open(\"../data/pan/string_850/k_sets_novel_2.0.pkl\", 'rb') as handle:\n",
    "    k_sets = pickle.load(handle)\n",
    "    \n",
    "e_data = torch.load(\"../data/pan/string_850/mut_data_miRNA_sub_du_go_path_2.0.pkl\")\n",
    "e_edge_index =e_data.edge_index\n",
    "e_edge_index = e_edge_index.to(device)\n",
    "\n",
    "#transformer layer 1 parameter settings\n",
    "d_model1 = 67  #dimension of input features\n",
    "d_ff1 = 268  #dimension mapped by feedforward neural network\n",
    "d_k1 = d_v1 = 67  # dimension of K(=Q), V\n",
    "\n",
    "#transformer layer 2 parameter settings\n",
    "d_model2 = 300  #dimension of input features\n",
    "d_ff2 = 1200  #dimension mapped by feedforward neural network\n",
    "d_k2 = d_v2 = 300  # dimension of K(=Q), V\n",
    "\n",
    "n_heads = 6  #number of heads in Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "364febff-eea7-4701-8d36-af7a2aac3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the framwork of Transformer layer 1\n",
    "class ScaledDotProductAttention1(nn.Module):\n",
    "    #use a single attention head to aggregate information\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention1, self).__init__()\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k1)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention1(nn.Module):\n",
    "    #use the multi-head attention mechanism to calculate the feature representations \n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention1, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model1, d_k1 * n_heads) \n",
    "        self.W_K = nn.Linear(d_model1, d_k1 * n_heads)\n",
    "        self.W_V = nn.Linear(d_model1, d_v1 * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v1, d_model1)\n",
    " \n",
    "    def forward(self, H):\n",
    "        residual, batch_size = H, H.size(0)\n",
    "        q_s = self.W_Q(H).view(batch_size, -1, n_heads, d_k1).transpose(1,2)\n",
    "        k_s = self.W_K(H).view(batch_size, -1, n_heads, d_k1).transpose(1,2)\n",
    "        v_s = self.W_V(H).view(batch_size, -1, n_heads, d_v1).transpose(1,2)\n",
    " \n",
    "        context = ScaledDotProductAttention1()(q_s, k_s, v_s)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v1)\n",
    "        output = self.linear(context)\n",
    "        return torch.relu(output + residual) #residual connection\n",
    "    \n",
    "class PoswiseFeedForwardNet1(nn.Module):\n",
    "    #the feedforward neural network of transformer layer 1\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet1, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model1, d_ff1, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(d_ff1, 300, bias=False))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.fc(inputs)\n",
    "        return output\n",
    "\n",
    "class TransformerLayer1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerLayer1, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention1()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet1()\n",
    " \n",
    "    def forward(self, enc_inputs):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs\n",
    "\n",
    "class TL1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TL1, self).__init__()\n",
    "        self.trans = TransformerLayer1()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        trans_inputs = inputs.unsqueeze(1)\n",
    "        trans_outputs = self.trans(trans_inputs)\n",
    "        outputs = trans_outputs.view(10743,-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ab1576-b3a1-4e0c-b0be-6c228402f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the framwork of Transformer layer 1\n",
    "class ScaledDotProductAttention1(nn.Module):\n",
    "    #use a single attention head to aggregate information\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention1, self).__init__()\n",
    " \n",
    "    def forward(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k1)\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention1(nn.Module):\n",
    "    #use the multi-head attention mechanism to calculate the feature representations \n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention1, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model1, d_k1 * n_heads) \n",
    "        self.W_K = nn.Linear(d_model1, d_k1 * n_heads)\n",
    "        self.W_V = nn.Linear(d_model1, d_v1 * n_heads)\n",
    "        self.linear = nn.Linear(n_heads * d_v1, d_model1)\n",
    " \n",
    "    def forward(self, H):\n",
    "        residual, batch_size = H, H.size(0)\n",
    "        q_s = self.W_Q(H).view(batch_size, -1, n_heads, d_k1).transpose(1,2)\n",
    "        k_s = self.W_K(H).view(batch_size, -1, n_heads, d_k1).transpose(1,2)\n",
    "        v_s = self.W_V(H).view(batch_size, -1, n_heads, d_v1).transpose(1,2)\n",
    " \n",
    "        context = ScaledDotProductAttention1()(q_s, k_s, v_s)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v1)\n",
    "        output = self.linear(context)\n",
    "        return torch.relu(output + residual) #residual connection\n",
    "    \n",
    "class PoswiseFeedForwardNet1(nn.Module):\n",
    "    #the feedforward neural network of transformer layer 1\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet1, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model1, d_ff1, bias=False),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(d_ff1, 300, bias=False))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = self.fc(inputs)\n",
    "        return output\n",
    "\n",
    "class TransformerLayer1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerLayer1, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention1()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet1()\n",
    " \n",
    "    def forward(self, enc_inputs):\n",
    "        enc_outputs = self.enc_self_attn(enc_inputs)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs\n",
    "\n",
    "class TL1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TL1, self).__init__()\n",
    "        self.trans = TransformerLayer1()\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        trans_inputs = inputs.unsqueeze(1)\n",
    "        trans_outputs = self.trans(trans_inputs)\n",
    "        outputs = trans_outputs.view(10743,-1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822e2066-b1b1-41a2-99b2-94e7635fa29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.T1 = TL1()\n",
    "        self.T2 = TL2()\n",
    "        self.GNN = ChebConv(100, 1, K=2, normalization=\"sym\")\n",
    "\n",
    "        self.lin11 = Linear(67, 300)\n",
    "        self.lin21 = Linear(67, 300)\n",
    "        self.lin31 = Linear(67, 300)\n",
    "        \n",
    "    def forward(self):\n",
    "        edge_index, _ = dropout_edge(e_edge_index, p=0.5,force_undirected=True,training=self.training)\n",
    "        x01 = F.dropout(data, training=self.training)\n",
    "        x02 = F.dropout(data2, training=self.training)\n",
    "        x03 = F.dropout(data3, training=self.training)\n",
    "        \n",
    "        #learn genes feature representations across various gene association networks by the transformer module\n",
    "        #apply weight sharing across the two Transformer layers\n",
    "        x1 = self.T1(x01)\n",
    "        x2 = torch.relu(x1 + self.lin11(x01)) #residual enhanced activation\n",
    "        x3 = F.dropout(x2, training=self.training)\n",
    "        x4 = self.T2(x3)\n",
    "        \n",
    "        x6 = self.T1(x02)\n",
    "        x7 = torch.relu(x6 + self.lin21(x02)) #residual enhanced activation\n",
    "        x8 = F.dropout(x7, training=self.training)\n",
    "        x9 = self.T2(x8)\n",
    "        \n",
    "        x11 = self.T1(x03)\n",
    "        x12 = torch.relu(x11 + self.lin31(x03)) #residual enhanced activation\n",
    "        x13 = F.dropout(x12, training=self.training)\n",
    "        x14 = self.T2(x13)\n",
    "        \n",
    "        #aggregate the three feature representation matrices and obtain predicted results by Chebyshev GCN\n",
    "        x19 = x4 + x9 + x14\n",
    "        x20 = F.dropout(x19, training=self.training)\n",
    "        x21 = self.GNN(x20, edge_index)\n",
    "        \n",
    "        return x21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac037d5-cf4d-4f95-a449-34a24161fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mask):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model()\n",
    "    loss = F.binary_cross_entropy_with_logits(pred[mask], Y[mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    x = model()\n",
    "    pred = torch.sigmoid(x[mask]).cpu().detach().numpy()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3f938-8625-47b7-8166-ef8772655a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 550\n",
    "time_start = time.time()\n",
    "\n",
    "tr_mask = k_sets[0]\n",
    "te_mask = k_sets[1]\n",
    "model = net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "for t in range(epochs):\n",
    "    train(tr_mask)   \n",
    "pred = test(te_mask) \n",
    "\n",
    "print(time.time() - time_start)\n",
    "np.savetxt(\"../data/pan/string_850/pred_novel_2.0.txt\",pred,fmt=\"%.10e\",delimiter=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myapp",
   "language": "python",
   "name": "myapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
